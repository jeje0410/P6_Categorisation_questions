{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a59eaae-73e2-4133-a203-92c4f8a63675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import date\n",
    "today = date.today()\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import sklearn\n",
    "from wordcloud import WordCloud\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import gensim\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8529d808-fc69-4171-b978-e1307cc51682",
   "metadata": {},
   "source": [
    "## Fonctions utlisées dans le code\n",
    "* Suppression des stopword\n",
    "* Lemmatisation : Finalement non utlisée car non appropriée à certains mots\n",
    "* Supression des tags qui ne sont pas dan le TOP 100\n",
    "* Traitement du C#\n",
    "* Fonction vide pour TF_IDF afin de ne pas utiliser les tokenizer natifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108c4bec-ddbe-4ca8-8bbc-e967543267e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fonction de preprocessing\n",
    "stop_words = set(stopwords.words('English'))\n",
    "def removeStopWord(Word_list):\n",
    "    filtered_Word_list = Word_list[:] #make a copy of the Word_list\n",
    "    for Word in Word_list: # iterate over Word_list\n",
    "        if Word.lower() in stop_words: \n",
    "            filtered_Word_list.remove(Word) # remove Word from filtered_Word_list if it is a stopword\n",
    "    return filtered_Word_list\n",
    "\n",
    "# Instantiate stemmers\n",
    "porter = PorterStemmer()\n",
    "#Fonction de lemmatisation\n",
    "def lemmatisation(Word_list):\n",
    "    Words = Word_list[:] #make a copy of the Word_list\n",
    "    Words = [porter.stem(word) for word in Words]\n",
    "    return Words\n",
    "\n",
    "#Fonction qui supprime le tag si celui ci n'appartient au TOP        \n",
    "def removeNotTop100(Word_list):\n",
    "    filtered_Word_list = Word_list[:] #make a copy of the Word_list\n",
    "    for Word in Word_list: # iterate over Word_list\n",
    "        if Word not in Top100: \n",
    "            filtered_Word_list.remove(Word) # remove Word from filtered_Word_list if it is a stopword\n",
    "    if len(filtered_Word_list) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return filtered_Word_list\n",
    "    \n",
    "#Fonction qui supprime le mot si seulement du numérique        \n",
    "def removeOnlyNumeric(Word_list):\n",
    "    word_list = Word_list[:] #make a copy of the Word_list\n",
    "    for Word in Word_list: # iterate over Word_list\n",
    "        if Word.isnumeric(): \n",
    "            word_list.remove(Word) # remove Word from filtered_Word_list if it is a stopword\n",
    "    return word_list\n",
    "    \n",
    "#Fonction qui traite le C#\n",
    "def processCSharp(Word_list):\n",
    "    word_list = Word_list[:] #make a copy of the Word_list\n",
    "    for index, value in enumerate(word_list):\n",
    "        if value == '#':\n",
    "            word_list.remove(value) # remove Word from filtered_Word_list if it is a stopword\n",
    "            word_list[index-1] = 'c#'\n",
    "    return word_list\n",
    "\n",
    "# Fonction qui ne fait rien pour exploiter le pré traitement que nous avons réalisé\n",
    "def dummy(doc):\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31629af5-e7c5-48b4-8e0f-c86942f1c9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lecture du Fichier\n",
    "df = pd.read_csv('posts_after_prepocessing.csv', sep = ',', encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4f93d7-b1f6-45b8-9436-ee42ee7c1298",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Préprocessing du champ Tittle et Body\n",
    "df['TitleBody'] = df['TitleBody'].map(lambda x: BeautifulSoup(x, \"html.parser\").get_text())\n",
    "df['TitleBody'] = df['TitleBody'].map(lambda x: x.replace('\\n', ' '))\n",
    "df['TitleBody'] = df['TitleBody'].map(lambda x: x.replace(':', ''))\n",
    "\n",
    "punct = string.punctuation\n",
    "for c in punct:\n",
    "    if c != '#':\n",
    "        df['TitleBody'] = df['TitleBody'].map(lambda x: x.replace(c, ''))\n",
    "\n",
    "#Création des nuages de mots avec NLTK\n",
    "df['TitleBody_final'] = df.apply(lambda row: nltk.word_tokenize(row['TitleBody'],language='english'), axis=1)\n",
    "\n",
    "#Supression des StopWord\n",
    "df['TitleBody_final'] = df.apply(lambda row:removeStopWord(row['TitleBody_final']), axis=1)\n",
    "\n",
    "#Lemmatisation\n",
    "#df['TitleBody_final'] = df.apply(lambda row:lemmatisation(row['TitleBody_final']), axis=1)\n",
    "\n",
    "#Traitement du C#\n",
    "df['TitleBody_final'] = df.apply(lambda row:processCSharp(row['TitleBody_final']), axis=1)\n",
    "\n",
    "#Suppressin des numériques\n",
    "df['TitleBody_final'] = df.apply(lambda row:removeOnlyNumeric(row['TitleBody_final']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701f96ee-9a1a-40a4-88e6-9aee848a599c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2fcf82-70e4-4d2c-b465-cc61f5762b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width = 800, height = 1000,\n",
    "                max_words=1000,\n",
    "                background_color ='white',\n",
    "                min_font_size = 12,\n",
    "                colormap=\"Set2\").generate(str(df['TitleBody_final']))\n",
    " \n",
    "# plot the WordCloud image                      \n",
    "plt.figure(figsize = (8, 8), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13be8ce5-f53c-4edf-8b3e-d02dbec07447",
   "metadata": {},
   "source": [
    "## Première étape on vectorize avec TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276716c0-2184-4dc8-9c28-8e833a5aed5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=12000,tokenizer=dummy,\n",
    "        preprocessor=dummy)\n",
    "tfidf = tfidf_vectorizer.fit_transform(df[\"TitleBody_final\"])\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b9ebe4-625f-4c81-abde-e688ee621907",
   "metadata": {},
   "source": [
    "### Selection du nombre de TOPIC\n",
    "* 'nndsvd': Nonnegative Double Singular Value Decomposition (NNDSVD) initialization (better for sparseness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5d0f71-9901-43cd-ad23-7b46b4e9b5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmin, kmax = 5, 20\n",
    "topic_models = []\n",
    "# try each value of k\n",
    "for k in range(kmin,kmax+1):\n",
    "    print(\"Applying NMF for k=%d ...\" % k )\n",
    "    # run NMF\n",
    "    model = NMF( init=\"nndsvd\", n_components=k,max_iter = 300,random_state=42) \n",
    "    W = model.fit_transform(tfidf)\n",
    "    H = model.components_    \n",
    "    # store for later\n",
    "    topic_models.append( (k,W,H) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f99328e-4070-4826-96fa-d4264fca22d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef880ad-bfc3-4c67-8ae0-4d469e31d007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_coherence(term_rankings ):\n",
    "    overall_coherence = 0.0\n",
    "    for topic_index in range(len(term_rankings)):\n",
    "        # check each pair of terms\n",
    "        pair_scores = []\n",
    "        for pair in combinations( term_rankings[topic_index], 2 ):\n",
    "            pair_scores.append(nlp(pair[0]).similarity(nlp(pair[1])))\n",
    "        # get the mean for all pairs in this topic\n",
    "        topic_score = sum(pair_scores) / len(pair_scores)\n",
    "        overall_coherence += topic_score\n",
    "    # get the mean score across all topics\n",
    "    return overall_coherence / len(term_rankings)\n",
    "\n",
    "def get_descriptor( all_terms, H, topic_index, top ):\n",
    "    # reverse sort the values to sort the indices\n",
    "    top_indices = np.argsort( H[topic_index,:] )[::-1]\n",
    "    # now get the terms corresponding to the top-ranked indices\n",
    "    top_terms = []\n",
    "    for term_index in top_indices[0:top]:\n",
    "        top_terms.append( all_terms[term_index] )\n",
    "    return top_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20db096a-751e-4584-8d21-70e912efe0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = []\n",
    "coherences = []\n",
    "for (k,W,H) in topic_models:\n",
    "    # Get all of the topic descriptors - the term_rankings, based on top 10 terms\n",
    "    term_rankings = []\n",
    "    for topic_index in range(k):\n",
    "        term_rankings.append( get_descriptor( tfidf_feature_names, H, topic_index, 10 ) )\n",
    "    k_values.append( k )\n",
    "    coherences.append( calculate_coherence(term_rankings ) )\n",
    "    print(\"K=%02d: Coherence=%.4f\" % ( k, coherences[-1] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019bb86b-c919-4b2d-bc62-adf44a489660",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(13,7))\n",
    "# create the line plot\n",
    "ax = plt.plot( k_values, coherences )\n",
    "plt.xticks(k_values)\n",
    "plt.xlabel(\"Number of Topics\")\n",
    "plt.ylabel(\"Mean Coherence\")\n",
    "# add the points\n",
    "plt.scatter(k_values, coherences, s=120)\n",
    "# find and annotate the maximum point on the plot\n",
    "ymax = max(coherences)\n",
    "xpos = coherences.index(ymax)\n",
    "best_k = k_values[xpos]\n",
    "plt.annotate(\"k=%d\" % best_k, xy=(best_k, ymax), xytext=(best_k, ymax), textcoords=\"offset points\", fontsize=16)\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3915c965-ccc0-41be-a24e-028844cf4fa6",
   "metadata": {},
   "source": [
    "## Puis on calcul les topics avec NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be44613-28d9-4e8c-9c72-497acc2947c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#On recupère le nombre de TOPIC optimal, c'est à dire 8\n",
    "from sklearn.decomposition import NMF\n",
    "k = 8\n",
    "# create the model, specifiying the initialization strategy and the number of topics to produce\n",
    "model = NMF(n_components=k,init=\"nndsvd\",random_state=42,max_iter = 300) \n",
    "# apply the model and extract the two factor matrices\n",
    "W = model.fit_transform(tfidf)\n",
    "H = model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f12dccd-4b4b-4273-a220-ddfcaea3a360",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_descriptor( terms, H, topic_index, top ):\n",
    "    # reverse sort the values to sort the indices\n",
    "    top_indices = np.argsort( H[topic_index,:] )[::-1]\n",
    "    # now get the terms corresponding to the top-ranked indices\n",
    "    top_terms = []\n",
    "    for term_index in top_indices[0:top]:\n",
    "        top_terms.append( tfidf_feature_names[term_index] )\n",
    "    return top_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3b9b92-e5e2-47a4-9122-729aaabcd8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptors = []\n",
    "for topic_index in range(k):\n",
    "    descriptors.append( get_descriptor( tfidf_feature_names, H, topic_index, 10 ) )\n",
    "    str_descriptor = \", \".join( descriptors[topic_index] )\n",
    "    print(\"Topic %02d: %s\" % ( topic_index+1, str_descriptor ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1427f55c-11af-4a86-8aaa-338906646e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, hamming_loss,jaccard_score\n",
    "#Fonction qui renvoi différents scores\n",
    "def print_score(y_test, y_pred):\n",
    "    print(\"Hamming loss : {}\".format(hamming_loss(y_test, y_pred)))\n",
    "    print('Subset Accuracy : ', accuracy_score(y_test, y_pred, normalize=True, sample_weight=None))\n",
    "    print('F1-score : ', f1_score(y_test, y_pred, average='micro'))\n",
    "    print('Jaccard : ', jaccard_score(y_test, y_pred, average='micro'))\n",
    "    #Revoir les indicateurs avec Jacard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa349e1b-ff8a-4ac7-b77e-5b113d5ee5ef",
   "metadata": {},
   "source": [
    "## Nommage TOPIC\n",
    "* Exemple de code\n",
    "* GIT\n",
    "* Traitement des chaines de caractères\n",
    "* Language Frontend (C#,HTML, CSS)\n",
    "* Shell\n",
    "* Gestion des tableaux\n",
    "* Base de données (SQL, MySQL)\n",
    "* Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58f69cf-83b6-4553-b894-a3f48f53814f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_term_weights( tfidf_feature_names, H, topic_index, top ):\n",
    "    # get the top terms and their weights\n",
    "    top_indices = np.argsort( H[topic_index,:] )[::-1]\n",
    "    top_terms = []\n",
    "    top_weights = []\n",
    "    for term_index in top_indices[0:top]:\n",
    "        top_terms.append( tfidf_feature_names[term_index] )\n",
    "        top_weights.append( H[topic_index,term_index] )\n",
    "    # note we reverse the ordering for the plot\n",
    "    top_terms.reverse()\n",
    "    top_weights.reverse()\n",
    "    # create the plot\n",
    "    fig = plt.figure(figsize=(13,8))\n",
    "    # add the horizontal bar chart\n",
    "    ypos = np.arange(top)\n",
    "    ax = plt.barh(ypos, top_weights, align=\"center\", color=\"green\",tick_label=top_terms)\n",
    "    plt.xlabel(\"Term Weight\",fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bf3f48-b358-46b9-975e-d5239c01bd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_top_term_weights(tfidf_feature_names, H, 0, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a4e02a-4095-456a-a76c-5a78db1c388f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_top_term_weights(tfidf_feature_names, H, 1, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e1d9e1-413d-478f-b79e-58538fa3cfa7",
   "metadata": {},
   "source": [
    "# LDA : Latent Dirichlet Allocation après bag of Words\n",
    "* Détermination des TOPICS\n",
    "Quels sont les thèmes, les distributions de chaque mot sur les thèmes, la fréquence d’apparition de chaque thème sur le corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9309ec8a-d55a-4d90-9c9b-116fd9bac361",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vectorizer = CountVectorizer(ngram_range = (1,1),\n",
    "                                 tokenizer=dummy,preprocessor=dummy)\n",
    "docs_bow = bow_vectorizer.fit_transform(df[\"TitleBody_final\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0554193f-ed45-4942-a905-9776bfdcdfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(random_state=42,\n",
    "                                batch_size=400)\n",
    "params = {'n_components' : [5, 6, 7,8],\n",
    "          \"learning_decay\" : [0.7, 0.9],\n",
    "          \"learning_method\" : [\"batch\", \"online\"]}\n",
    "\n",
    "#params = {'n_components' : [6,7,8]}\n",
    "\n",
    "gridsearch_lda = GridSearchCV(lda,\n",
    "                              param_grid=params,\n",
    "                              cv=5,\n",
    "                              verbose=2)\n",
    "gridsearch_lda.fit(docs_bow)\n",
    "\n",
    "#1 heure d'exécution. Résultat obtenu\n",
    "#Best Model's Params:  {'learning_decay': 0.9, 'learning_method': 'online', 'n_components': 8}\n",
    "#Best Log Likelihood Score:  -1689928.643148844"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aedf2d7-a94b-430d-9d9f-a448c2d0d164",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lda_model = gridsearch_lda.best_estimator_\n",
    "\n",
    "print(\"Best Model's Params: \", gridsearch_lda.best_params_)\n",
    "print(\"Best Log Likelihood Score: \", gridsearch_lda.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf5567d-09c0-4502-af33-797e1835fd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "panel = pyLDAvis.sklearn.prepare(best_lda_model, docs_bow, bow_vectorizer, mds='tsne')\n",
    "pyLDAvis.display(panel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59706edf-c88e-4ff1-a27d-5c66fb5acee5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
